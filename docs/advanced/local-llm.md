# ローカルLLMを使う

ローカルLLMの使用は、上級者向けの上、非常に高価なPCが必要です。

本アプリケーションを利用する際に必要な設定を端的に説明します。

## BaseURL (Endpoint)
LM Studio: `http://127.0.0.1:1234/v1`  
Ollama: `http://127.0.0.1:11434/v1`  

ポート番号は適切に設定・変更してください。

## APIキー
LM Studio、OllamaはAPIキーを必要としませんが、本アプリケーションはAPIキーの入力を必須としています。  
(未入力の場合は、ようこそ画面に遷移してしまいます)

そのため、適当な文字列を入力してください。1文字でも構いません。

## モデル
gpt-oss-20b、qwen3などで動作を確認しています。  

本アプリケーションの性質上、コンテキストサイズが広く必要です。(推奨128k以上)  
また、Chat Templateが使用できるモデルを選択してください。(事前学習モデルではなく、インストラクションチューニング済みモデル)  
XML形式のプロンプトを理解できるモデルである必要があります。

## 設定の調整
快適に使用するには、応用的な設定から、以下の設定を調整してください。

+ 最大トークン長(Max Tokens)
+ 通信タイムアウト

## 補足
LM Studio、Ollamaは、インストール直後の状態では、外部からの接続を受け付けない設定になっています。  
同一PC内からの接続は可能ですが、必要に応じて各ソフトウェア側の設定を変更してください。

